{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Languages are made up of several words often derived from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\"\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Inflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.\n",
    "*Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are English and Non-English Stemmers available in `nltk` package.\n",
    "For the English language, you can choose between [PorterStemmer](https://tartarus.org/martin/PorterStemmer/def.txt) or LancasterStemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "words = ['dogs', 'trouble', 'troubling', 'troubled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStemmer uses **suffix stripping** to produce stems. But if you look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because **PorterStemmer algorithm does not follow linguistics rather a set of 5 rules for different cases that are applied in phases (step by step) to generate stems**. This is the reason why PorterStemmer does not often generate stems that are actual English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is known for its **simplicity** and **speed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **LancasterStemmer (Paice-Husk stemmer)** is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in code below `destabilized` is stemmed to `dest` in LancasterStemmer whereas using PorterStemmer `destabl`. LancasterStemmer produces an even shorter stem than porter because of iterations and over-stemming is occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter              Lancaster           \n",
      "\n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabilize           stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "# Comparison of Porter and Lancaster stemmers\n",
    "word_list = [\"friend\", \"friendship\", \"friends\",\"friendships\",\"stabilize\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(f'{\"Word\":20}{\"Porter\":20}{\"Lancaster\":20}')\n",
    "print()\n",
    "\n",
    "for word in word_list:\n",
    "    print(f\"{word:20}{porter.stem(word):20}{lancaster.stem(word):20}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To separate the sentence into words, you can use **tokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentence = \"All work and no play makes jack a dull boy, all work and no play.\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all work and no play make jack a dull boy , all work and no play .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English Stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` provides **SnowballStemmers, ISRIStemmer, RSLPSStemmer**.\n",
    "\n",
    "**ISRIStemmer** is an Arabic stemmer and **RSLPStemmer** is stemmer for the Portuguese Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "# SnowballStemmer languages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print('\\n'.join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.stem('sleeping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tell the stemmer to ignore stop-words.\n",
    "\n",
    ">Stop Words are words which do not contain important significance to be used. Usually, these words are filtered out because they return a vast amount of unnecessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "stemmer.stem('sleeping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dorm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
    "stemmer.stem(\"dormir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.'\n",
    "punctuations = '?:!.,;'\n",
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = []\n",
    "\n",
    "for word in tokens:\n",
    "    if not word in punctuations:\n",
    "        sentence_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Word\":20}{\"Lemma\":20}')\n",
    "for word in sentence_words:\n",
    "    print(f'{word:20}{wordnet_lemmatizer.lemmatize(word):20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for `pos` parameter in `wordnet_lemmatizer.lemmatize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Word\":20}{\"Lemma\":20}')\n",
    "for word in sentence_words:\n",
    "    print(f'{word:20}{wordnet_lemmatizer.lemmatize(word, pos=\"v\"):20}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
